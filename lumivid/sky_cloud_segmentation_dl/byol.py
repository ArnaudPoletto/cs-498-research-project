# Ensure that the current working directory is this file
import sys
from pathlib import Path
GLOBAL_DIR = Path(__file__).parent / '..' / '..'
sys.path.append(str(GLOBAL_DIR))

import os
import numpy as np
from tqdm import tqdm
from PIL import Image
from torch.cuda.amp import GradScaler, autocast
from pixel_level_contrastive_learning import PixelCL

import torch
import torch.nn as nn
from torchvision.models import resnet101
import torchvision.models.segmentation as models

from lumivid.utils.random_utils import set_seed

DATA_PATH = str(GLOBAL_DIR / 'data') + '/'
SCS_DATA_PATH = DATA_PATH + "sky_cloud_segmentation/"

SKY_FINDER_PATH = SCS_DATA_PATH + "sky_finder/"
SKY_FINDER_INPUTS_PATH = SKY_FINDER_PATH + "images/"
SKY_FINDER_MASKS_PATH = SKY_FINDER_PATH + "masks/"

BYOL_MODEL_PATH = SCS_DATA_PATH + "models/byol.pth"

N_PATCHES = 5
IMAGE_SIZE = 50
SAMPLE_SIZE = 10
EPOCHS = 10_000
LEARNING_RATE = 0.0001

IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

SEED = 27

def get_byol_data(n_patches: int, patch_size: int) -> torch.tensor:
    """
    Get BYOL data, which consists of patches of images from the Sky Finder dataset.

    Args:
        n_patches (int): Number of patches to extract from each image.
        patch_size (int): Size of each patch.

    Returns:
        torch.tensor: BYOL data.
    """
    input_paths = []
    mask_paths = []

    # Get Sky Finder paths
    input_folder_paths = sorted([os.path.join(SKY_FINDER_INPUTS_PATH, folder) for folder in os.listdir(SKY_FINDER_INPUTS_PATH)])
    for folder_path in input_folder_paths:
        image_paths = sorted([os.path.join(folder_path, filename) for filename in os.listdir(folder_path)])
        for image_path in image_paths:
            input_paths.append(image_path)
            mask_paths.append(os.path.join(SKY_FINDER_MASKS_PATH, folder_path.split("/")[-1] + ".png"))
    print("➡️ Number of SkyFinder images:", len(input_paths))

    # Read data into memory
    patches = []
    for input_path, mask_path in tqdm(zip(input_paths, mask_paths), total=len(input_paths)):
        # Read mask and image
        image = np.array(Image.open(input_path))
        mask = np.array(Image.open(mask_path))

        # Apply mask on image
        image[mask == 0] = 0

        # Extract random patches
        h, w, _ = image.shape
        for _ in range(n_patches):
            top = np.random.randint(0, h - patch_size)
            left = np.random.randint(0, w - patch_size)
            patch = image[top:top+patch_size, left:left+patch_size]
            patches.append(patch)

    # To tensor and normalize
    patches = torch.tensor(np.array(patches)).permute(0, 3, 1, 2).float()
    patches = patches / 255.0
    patches = patches - torch.tensor(IMAGENET_MEAN).view(1, 3, 1, 1)
    patches = patches / torch.tensor(IMAGENET_STD).view(1, 3, 1, 1)

    print("✅ Generated BYOL data of shape:", patches.shape)

    return patches

def get_model():
    """
    def modify_resnet_for_deeplab(model):
        model.layer3[0].conv2.stride = (1, 1)
        model.layer3[0].downsample[0].stride = (1, 1)
        for i in range(1, 23):
            model.layer3[i].conv2.padding = (2, 2)
            model.layer3[i].conv2.dilation = (2, 2)

        model.layer4[0].conv2.stride = (1, 1)
        model.layer4[0].conv2.padding = (2, 2)
        model.layer4[0].conv2.dilation = (2, 2)
        model.layer4[0].downsample[0].stride = (1, 1)
        for i in range(1, 3):
            model.layer4[i].conv2.padding = (4, 4)
            model.layer4[i].conv2.dilation = (4, 4)
        
    resnet101_model = resnet101(weights=None)
    modify_resnet_for_deeplab(resnet101_model)
    """
    
    # Get model from DeepLabV3 backbone
    model = models.deeplabv3_resnet50(
        weights='COCO_WITH_VOC_LABELS_V1',
        weights_backbone='IMAGENET1K_V2',
        progress=True, 
        num_classes=21, 
        aux_loss=True
    ).to(DEVICE)
    resnet50_model = model.backbone

    # remove last 2 layers TODO


    return resnet50_model

if __name__ == '__main__':
    # Set seed for deterministic results
    set_seed(SEED)

    # Extract the backbone from deeplabv3_resnet101 model
    model = get_model()

    # Initialize BYOL
    learner = PixelCL(
        model,
        image_size = IMAGE_SIZE,
        hidden_layer_pixel = 'layer4',  # leads to output of 8x8 feature map for pixel-level learning
        hidden_layer_instance = -2,     # leads to output for instance-level learning
        projection_size = 256,          # size of projection output, 256 was used in the paper
        projection_hidden_size = 2048,  # size of projection hidden dimension, paper used 2048
        moving_average_decay = 0.99,    # exponential moving average decay of target encoder
        ppm_num_layers = 1,             # number of layers for transform function in the pixel propagation module, 1 was optimal
        ppm_gamma = 2,                  # sharpness of the similarity in the pixel propagation module, already at optimal value of 2
        distance_thres = 0.7,           # ideal value is 0.7, as indicated in the paper, which makes the assumption of each feature map's pixel diagonal distance to be 1 (still unclear)
        similarity_temperature = 0.3,   # temperature for the cosine similarity for the pixel contrastive loss
        alpha = 1.,                      # weight of the pixel propagation loss (pixpro) vs pixel CL loss
        use_pixpro = True,               # do pixel pro instead of pixel contrast loss, defaults to pixpro, since it is the best one
        cutout_ratio_range = (0.6, 0.8)  # a random ratio is selected from this range for the random cutout
    ).to(DEVICE)
    
    optimizer = torch.optim.Adam(learner.parameters(), lr=LEARNING_RATE)

    # Get data
    images = get_byol_data(n_patches=N_PATCHES, patch_size=IMAGE_SIZE)
    n_images = images.shape[0]

    # Train
    scaler = GradScaler()
    bar = tqdm(range(EPOCHS))
    for _ in range(EPOCHS):
        idx = np.random.choice(n_images, SAMPLE_SIZE, replace=False)
        sampled_images = images[idx].to(DEVICE)

        with autocast():
            loss, positive_pairs = learner(sampled_images, return_positive_pairs=True)

        optimizer.zero_grad()
        scaler.scale(loss).backward() # loss.backward()
        scaler.unscale_(optimizer)
        scaler.step(optimizer)
        scaler.update()
        learner.update_moving_average()

        bar.set_description(f"loss: {loss.item():.5f}, positive pairs: {int(positive_pairs.item())}")

        # Delete variables to free up memory
        del sampled_images
        del loss
        del positive_pairs

        bar.update(1)

    # Save model
    torch.save(model.state_dict(), BYOL_MODEL_PATH)
    print("✅ Model saved to ", BYOL_MODEL_PATH)